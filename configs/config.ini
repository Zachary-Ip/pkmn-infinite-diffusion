[settings]
dataset_name = None
dataset_path = data/raw/
metadata_path = data/metadata/

output_dir = trained_models/
samples_dir = test_samples/
loss_logs_dir = training_logs
cache_dir = None
logging_dir = logs
pretrained_model_path = None
resolution = 128
hidden_dims = [8, 16, 32, 64, 128]
n_timesteps = 1000
n_inference_timesteps = 500
use_flash_attn = True

train_batch_size = 32
eval_batch_size = 32
num_epochs = 10
save_model_steps = 2500
gradient_accumulation_steps = 1
learning_rate = 0.0001
lr_scheduler = cosine
lr_warmup_steps = 1000
adam_beta1 = 0.9
adam_beta2 = 0.99
adam_weight_decay = 0.0
use_clip_grad = True
fp16_precision = True
gamma = 0.996
guidance_scale = 3.0
